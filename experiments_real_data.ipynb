{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T17:08:51.199533Z",
     "start_time": "2021-01-18T17:08:51.192962Z"
    }
   },
   "source": [
    "# Pairwise Adjusted Mutual Information\n",
    "\n",
    "# Experiments on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the experiments on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:28:17.670665Z",
     "start_time": "2021-01-08T18:28:17.435233Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:37:11.858600Z",
     "start_time": "2021-01-18T18:37:11.847311Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics.cluster._expected_mutual_info_fast import expected_mutual_information\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:36:34.363807Z",
     "start_time": "2021-01-18T18:36:34.358670Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following benchmark, consisting of **79** different datasets:\n",
    "\n",
    "M. Gagolewski and others (Eds.), Benchmark Suite for Clustering Algorithms -- Version 1, 2020\n",
    "\n",
    "https://github.com/gagolews/clustering_benchmarks_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gagolews/clustering_benchmarks_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:37:15.371869Z",
     "start_time": "2021-01-18T18:37:15.355965Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./clustering_benchmarks_v1/\"\n",
    "data_files = natsorted([f for f in glob.glob(f\"{path}*/*.data.gz\")])\n",
    "dataset_names = [file.split(path)[1].split('.')[0] for file in data_files]\n",
    "dataset_names = [name for name in dataset_names if not (('g2mg' in name) or ('h2mg' in name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Adjusted Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:36:35.826333Z",
     "start_time": "2021-01-18T18:36:35.816191Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_adjusted_mutual_info_pair(contingency, n_samples):\n",
    "    \"\"\"Return pairwise adjusted mutual information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    contingency: np.ndarray\n",
    "        Contingency matrix\n",
    "    n_samples : int\n",
    "        Number of samples\n",
    "    \"\"\"\n",
    "    k, l = contingency.shape\n",
    "    a = contingency.sum(axis=1)\n",
    "    b = contingency.sum(axis=0)\n",
    "    c = contingency.ravel()\n",
    "    # first term\n",
    "    factor = c * (contingency - np.outer(a, np.ones(l)) - np.outer(np.ones(k), b) + n_samples).ravel()\n",
    "    entropy = np.zeros(len(c))\n",
    "    entropy[c > 0] = c[c > 0] / n_samples * np.log(c[c > 0] / n_samples)\n",
    "    entropy_ = np.zeros(len(c))\n",
    "    entropy_[c > 1] = (c[c > 1] - 1) / n_samples * np.log((c[c > 1] - 1) / n_samples)\n",
    "    result = np.sum(factor * (entropy - entropy_)) / n_samples ** 2\n",
    "    # second term\n",
    "    factor = ((np.outer(a, np.ones(l)) - contingency) * (np.outer(np.ones(k), b) - contingency)).ravel()\n",
    "    entropy_ = (c + 1) / n_samples * np.log((c + 1) / n_samples)\n",
    "    result += np.sum(factor * (entropy - entropy_)) / n_samples ** 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Adjusted Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:36:36.696729Z",
     "start_time": "2021-01-18T18:36:36.685873Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_adjusted_mutual_info_exact(contingency, n_samples):\n",
    "    \"\"\"Return adjusted mutual information (without normalization).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    contingency: np.ndarray\n",
    "        Contingency matrix\n",
    "    n_samples : int\n",
    "        Number of samples\n",
    "    \"\"\"\n",
    "    mi = mutual_info_score(_, _, contingency=contingency)\n",
    "    emi = expected_mutual_information(contingency, n_samples)\n",
    "    result = mi - emi\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:59:05.500224Z",
     "start_time": "2021-01-18T18:59:05.497119Z"
    }
   },
   "source": [
    "We consider the clustering algorithms of scikit-learn.\n",
    "\n",
    "In order to evaluate similarity between results obtained with **Full Adjusted Mutual Information** and **Pairwise Adjusted Mutual Information**, we use the **Spearman correlation** between rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:39:34.564416Z",
     "start_time": "2021-01-18T18:39:34.539125Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_algorithms(X):\n",
    "    params = default_base.copy()\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "                            xi=params['xi'],\n",
    "                            min_cluster_size=params['min_cluster_size'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "    return (\n",
    "                ('MiniBatchKMeans', two_means),\n",
    "                ('AffinityPropagation', affinity_propagation),\n",
    "                ('MeanShift', ms),\n",
    "                ('SpectralClustering', spectral),\n",
    "                ('Ward', ward),\n",
    "                ('AgglomerativeClustering', average_linkage),\n",
    "                ('DBSCAN', dbscan),\n",
    "                ('OPTICS', optics),\n",
    "                ('Birch', birch),\n",
    "                ('GaussianMixture', gmm)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducible results\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T18:39:36.195323Z",
     "start_time": "2021-01-18T18:39:36.188354Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_samples = []\n",
    "nb_labels = []\n",
    "results = []\n",
    "gains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(dataset_names):\n",
    "    \n",
    "    X = np.loadtxt(path + name +\".data.gz\", ndmin=2)\n",
    "    y = np.loadtxt(path + name +\".labels0.gz\", dtype=np.intc)\n",
    "    n_samples, n_features = X.shape\n",
    "    nb_samples.append(n_samples)\n",
    "    nb_labels.append(len(set(y)))\n",
    "    \n",
    "    print(i + 1, '/', len(dataset_names), name, n_samples)\n",
    "    \n",
    "    if n_features > 100:\n",
    "        # dimension reduction\n",
    "        svd = TruncatedSVD(n_components=10)\n",
    "        X = svd.fit_transform(X)\n",
    "\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    clustering_algorithms = prepare_algorithms(X)\n",
    "\n",
    "    sim_full = []\n",
    "    sim_pair = []\n",
    "    time_full = []\n",
    "    time_pair = []\n",
    "\n",
    "    for algo_name, algorithm in clustering_algorithms:\n",
    "        algorithm.fit(X)\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "        contingency = contingency_matrix(y, y_pred)\n",
    "        t0 = time.time()\n",
    "        sim_full.append(get_adjusted_mutual_info_exact(contingency, len(y)))\n",
    "        t1 = time.time()\n",
    "        time_full.append(t1 - t0)\n",
    "        t0 = time.time()\n",
    "        sim_pair.append(get_adjusted_mutual_info_pair(contingency, len(y)))\n",
    "        t1 = time.time()        \n",
    "        time_pair.append(t1 - t0)\n",
    "\n",
    "    result = stats.spearmanr(sim_full, sim_pair).correlation\n",
    "    gain = np.mean(np.array(time_full) / np.array(time_pair))\n",
    "\n",
    "    results.append(result)\n",
    "    gains.append(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of datasets with correlation > 0.95\n",
    "np.sum(np.array(results) > 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argsort(nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1 + np.arange(len(results)), np.array(results)[index], c='b', lw=3)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Spearman correlation')\n",
    "plt.savefig('spearman.pdf', bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1 + np.arange(len(gains)), np.array(gains)[index], c='b', lw=3)\n",
    "plt.ylim(0, 20)\n",
    "plt.yticks([0,5, 10,15, 20])\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Speed-up')\n",
    "plt.savefig('speedup.pdf', bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
